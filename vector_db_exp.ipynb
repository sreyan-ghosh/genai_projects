{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to leverage the Cohere Embeddings generator to create our embeddings and then use Pinecone to store the vectors before using the Cohere API again to create a QA on a PDF file that we have chosen to be our source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain.llms import Cohere\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from ApiSecrets import ApiSecrets\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PDF Directory as our Retrieval Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "source_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "text_chunks = text_splitter.split_documents(source_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "os.environ[\"COHERE_API_KEY\"] = ApiSecrets.COHERE_API_KEY\n",
    "os.environ[\"PINECONE_API_KEY\"] = ApiSecrets.PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\opensource\\genai_projects\\genai\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.cohere.CohereEmbeddings` was deprecated in langchain-community 0.0.30 and will be removed in 0.2.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
    "text = \"this is a test document\"\n",
    "sample_embed = embeddings.embed_query(text)\n",
    "len(sample_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "index_name = \"testing-vec-db\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings from each chunk from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as LC_Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Pinecone', 'CohereEmbeddings'], vectorstore=<langchain_community.vectorstores.pinecone.Pinecone object at 0x000001C82DF99010>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecstore = LC_Pinecone.from_texts([chunk.page_content for chunk in text_chunks], embeddings, index_name=index_name)\n",
    "vecstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      " Score: 0.599071622\n"
     ]
    }
   ],
   "source": [
    "simi_prompt = \"what is attention?\"\n",
    "simi_result = vecstore.similarity_search_with_score(simi_prompt)\n",
    "print(f\"Answer: {simi_result[0][0].page_content}\\n Score: {simi_result[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\opensource\\genai_projects\\genai\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.cohere.Cohere` was deprecated in langchain-community 0.1.14 and will be removed in 0.2.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import Cohere`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = Cohere(cohere_api_key = os.getenv(\"COHERE_API_KEY\"))\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vecstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The methodological strengths of the provided text lie in the thorough and insightful analysis of the performance of a particular model for parsing tasks. The analysis compares the model's performance to other previously reported models in the field, noting where it outperforms them. \n",
      "\n",
      "The text additionally highlights the benefits of the model's attention mechanism, which enables the model to handle long-range dependencies and capture global patterns, therefore improving its performance. \n"
     ]
    }
   ],
   "source": [
    "qa_prompt = \"what is multi-head attention?\"\n",
    "qa_result = qa.run(qa_prompt)\n",
    "print(qa_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to quit\n",
      "Ans:  From the provided context, a Transformer is a model architecture that substitutes recurrence and self-attention mechanisms for drawing global dependencies between input and output. Specifically, The Transformer presents an updated approach to sequence transduction models that utilize multi-headed self-attention mechanisms, replacing the use of recurrent layers in encoder-decoder architectures. The model allows for more parallelization and can reach a new state of the art in translation tasks. \n",
      "Ans:  This paper was presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017) and was published on August 2, 2023, as stated in the copyright section of the paper. \n",
      "The authors' names and affiliations are listed on the paper, and the identity of the specific author who wrote the paper may be included in this information in some cases. \n",
      "However, I don't have access to real-time data on the internet, so I cannot search for any subsequent updates to the paper or any later annotations. \n",
      "\n",
      "If you provide me with the names of the authors, I can tell you whether they are included in the aforementioned list that is provided in the paper. \n",
      "\n",
      "Can I assist you with anything else? \n",
      "Ans:  The provided text describes the premise of the attention paper as focusing on presenting a variety of attention mechanisms and arguing for the usefulness of self-attention in sequence transduction tasks. \n",
      "They describe a multitude of attention types, including intra-attention or self-attention, describing relations between positions in a single sequence. This mechanism allows the model to pay attention to different representation subspaces at different positions, giving it the ability to follow long-distance dependencies, as shown in Figure 3. \n",
      "The paper supports the argument for the usefulness of attention in sequence transduction tasks, used successfully in reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations, etc. \n",
      "\n",
      "Let me know if you'd like any clarification on the provided text or if there are any other questions I can help with! \n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to quit\")\n",
    "while True:\n",
    "    user_input = input(\"Enter Prompt: \")\n",
    "    if user_input == \"exit\" or user_input == \"Exit\":\n",
    "        break\n",
    "    if user_input == '':\n",
    "        continue\n",
    "    res = qa({\"query\": user_input})\n",
    "    print(f\"Ans: {res[\"result\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download news article data\n",
    "Download Commands:\n",
    "- Windows: `Invoke-WebRequest -Uri \"https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\" -OutFile \"new_articles.zip\"`\n",
    "- Unix: `wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip`\n",
    "\n",
    "Unzip Commands:\n",
    "- Windows: `Expand-Archive -Path \"new_articles.zip\" -DestinationPath \"news_articles\"`\n",
    "- Unix: `unzip -q new_articles.zip -d new_articles`\n",
    "\n",
    "<b>NOTE:</b> These commands are correct. But this is not working correctly in this case, maybe due to file format issues in Dropbox. Here, I have done it manually using GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files into list of Documents\n",
    "We can use `DirectoryLoader` paired with `TextLoader` to complete the task in one line, but due to some encoding errors, I was not able to process it using that path. Explicit encoding declaration was required and so I chose the path used below. In Unix systems, this is usually not an issue and the following command can be used to do the same job with prebuilt funcs: \n",
    "```python\n",
    "docs = DirectoryLoader(\"./news_articles\", glob=\"./*.txt\", loader_cls=TextLoader).load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"./news_articles\"\n",
    "txt_files = os.listdir(dirpath)\n",
    "filelist = [file for file in txt_files if file.endswith(\".txt\")]\n",
    "docs = []\n",
    "for file in filelist:\n",
    "    filepath = os.path.join(dirpath, file)\n",
    "    filetext = TextLoader(filepath, encoding='utf-8').load()[0]\n",
    "    docs.append(filetext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signaling that investments in the supply chain sector remain robust, Pando, a startup developing fulfillment management technologies, today announced that it raised $30 million in a Series B round, bringing its total raised to $45 million.\n",
      "\n",
      "Iron Pillar and Uncorrelated Ventures led the round, with participation from existing investors Nexus Venture Partners, Chiratae Ventures and Next47. CEO and founder Nitin Jayakrishnan says that the new capital will be put toward expanding Pando’s global sales, marketing and delivery capabilities.\n",
      "\n",
      "“We will not expand into new industries or adjacent product areas,” he told TechCrunch in an email interview. “Great talent is the foundation of the business — we will continue to augment our teams at all levels of the organization. Pando is also open to exploring strategic partnerships and acquisitions with this round of funding.”\n"
     ]
    }
   ],
   "source": [
    "print(text[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Chroma Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_dirname = \"vecdb\"\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1c82f7c9b20>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=persist_dirname, embedding_function=embeddings)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating QA Retriever using Chroma VecDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "prompt_res = retriever.get_relevant_documents(\"What is the relation between databricks and okera?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks today announced that it has acquired Okera, a data governance platform with a focus on AI. The two companies did not disclose the purchase price. According to Crunchbase, Okera previously raised just under $30 million. Investors include Felicis, Bessemer Venture Partners, Cyber Mentor Fund, ClearSky and Emergent Ventures.\n",
      "\n",
      "Data governance was already a hot topic, but the recent focus on AI has highlighted some of the shortcomings of the previous approach to it, Databricks notes in today’s announcement. “Historically, data governance technologies, regardless of sophistication, rely on enforcing control at some narrow waist layer and require workloads to fit into the ‘walled garden’ at this layer,” the company explains in a blog post. That approach doesn’t work anymore in the age of large language models (LLMs) because the number of assets is growing too quickly (in part because so much of it is machine-generated) and because the overall AI landscape is changing so quickly, standard access controls aren’t able to capture these changes quickly enough.\n",
      "\n",
      "Okera then uses an AI-powered system that can automatically discover and classify personally identifiable information, tag it and apply rules to this (with a focus on the metadata), using a no-code interface.\n",
      "\n",
      "As the Databricks team stressed, that’s one of the reasons the company was interested in acquiring Okera, but the other is the service’s isolation technology, which can enforce governance control on arbitrary workloads without any major overhead. This technology is still in private preview but was likely one of the major reasons Databricks acquired the company.\n",
      "\n",
      "Databricks, which launched its own LLM a few weeks ago, plans to integrate Okera’s technology into its Unity Catalog, its existing governance solution of data and AI assets. The company also noted that the acquisition will enable Databricks to expose additional APIs that its own data governance partners will be able to use to provide solutions to their customers.\n",
      "\n",
      "With this acquisition, Databricks is also bringing Okera co-founder and CEO Nong Li on board. Li created the Apache Parquet data storage format and was actually briefly an engineer at Databricks between working at Cloudera and before starting Okera, where he was the founding CTO and became the CEO in February 2022.\n",
      "\n",
      "“As data continues to grow in volume, velocity, and variety across different applications, CIOs, CDOs, and CEOs across the board have to balance those two often conflicting initiatives – not to mention that historically, managing access policies across multiple clouds has been painful and time-consuming,” writes Li in today’s announcement. “Many organizations don’t have enough technical talent to manage access policies at scale, especially with the explosion of LLMs. What they need is a modern, AI-centric governance solution. We could not be more excited to join the Databricks team and to bring our expertise in building secure, scalable and simple governance solutions for some of the world’s most forward-thinking enterprises.”\n",
      "\n",
      "If you know more about this acquisition, you can contact Frederic on Signal at (860) 208-3416 or by email (frederic@techcrunch.com). You can also reach us via SecureDrop.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'k': 2}\n",
      "Databricks today announced that it has acquired Okera, a data governance platform with a focus on AI. The two companies did not disclose the purchase price. According to Crunchbase, Okera previously raised just under $30 million. Investors include Felicis, Bessemer Venture Partners, Cyber Mentor Fund, ClearSky and Emergent Ventures.\n",
      "\n",
      "Data governance was already a hot topic, but the recent focus on AI has highlighted some of the shortcomings of the previous approach to it, Databricks notes in today’s announcement. “Historically, data governance technologies, regardless of sophistication, rely on enforcing control at some narrow waist layer and require workloads to fit into the ‘walled garden’ at this layer,” the company explains in a blog post. That approach doesn’t work anymore in the age of large language models (LLMs) because the number of assets is growing too quickly (in part because so much of it is machine-generated) and because the overall AI landscape is changing so quickly, standard access controls aren’t able to capture these changes quickly enough.\n",
      "\n",
      "Okera then uses an AI-powered system that can automatically discover and classify personally identifiable information, tag it and apply rules to this (with a focus on the metadata), using a no-code interface.\n",
      "\n",
      "As the Databricks team stressed, that’s one of the reasons the company was interested in acquiring Okera, but the other is the service’s isolation technology, which can enforce governance control on arbitrary workloads without any major overhead. This technology is still in private preview but was likely one of the major reasons Databricks acquired the company.\n",
      "\n",
      "Databricks, which launched its own LLM a few weeks ago, plans to integrate Okera’s technology into its Unity Catalog, its existing governance solution of data and AI assets. The company also noted that the acquisition will enable Databricks to expose additional APIs that its own data governance partners will be able to use to provide solutions to their customers.\n",
      "\n",
      "With this acquisition, Databricks is also bringing Okera co-founder and CEO Nong Li on board. Li created the Apache Parquet data storage format and was actually briefly an engineer at Databricks between working at Cloudera and before starting Okera, where he was the founding CTO and became the CEO in February 2022.\n",
      "\n",
      "“As data continues to grow in volume, velocity, and variety across different applications, CIOs, CDOs, and CEOs across the board have to balance those two often conflicting initiatives – not to mention that historically, managing access policies across multiple clouds has been painful and time-consuming,” writes Li in today’s announcement. “Many organizations don’t have enough technical talent to manage access policies at scale, especially with the explosion of LLMs. What they need is a modern, AI-centric governance solution. We could not be more excited to join the Databricks team and to bring our expertise in building secure, scalable and simple governance solutions for some of the world’s most forward-thinking enterprises.”\n",
      "\n",
      "If you know more about this acquisition, you can contact Frederic on Signal at (860) 208-3416 or by email (frederic@techcrunch.com). You can also reach us via SecureDrop.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={'k':2})\n",
    "prompt_res = retriever.get_relevant_documents(\"What is the relation between databricks and okera?\")\n",
    "print(len(prompt_res))\n",
    "print(retriever.search_kwargs)\n",
    "print(prompt_res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cited_answer(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\opensource\\genai_projects\\genai\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the relation between databricks and okera?\"\n",
    "response = qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the relation between databricks and okera?',\n",
       " 'result': \" Databricks has recently acquired Okera, a data governance platform. Databricks plans to integrate Okera's technology into its own platform, specifically its Unity Catalog. The acquisition will also enable Databricks to expose additional APIs that its data governance partners can use to provide solutions for their customers. \",\n",
       " 'source_documents': [Document(page_content='Databricks today announced that it has acquired Okera, a data governance platform with a focus on AI. The two companies did not disclose the purchase price. According to Crunchbase, Okera previously raised just under $30 million. Investors include Felicis, Bessemer Venture Partners, Cyber Mentor Fund, ClearSky and Emergent Ventures.\\n\\nData governance was already a hot topic, but the recent focus on AI has highlighted some of the shortcomings of the previous approach to it, Databricks notes in today’s announcement. “Historically, data governance technologies, regardless of sophistication, rely on enforcing control at some narrow waist layer and require workloads to fit into the ‘walled garden’ at this layer,” the company explains in a blog post. That approach doesn’t work anymore in the age of large language models (LLMs) because the number of assets is growing too quickly (in part because so much of it is machine-generated) and because the overall AI landscape is changing so quickly, standard access controls aren’t able to capture these changes quickly enough.\\n\\nOkera then uses an AI-powered system that can automatically discover and classify personally identifiable information, tag it and apply rules to this (with a focus on the metadata), using a no-code interface.\\n\\nAs the Databricks team stressed, that’s one of the reasons the company was interested in acquiring Okera, but the other is the service’s isolation technology, which can enforce governance control on arbitrary workloads without any major overhead. This technology is still in private preview but was likely one of the major reasons Databricks acquired the company.\\n\\nDatabricks, which launched its own LLM a few weeks ago, plans to integrate Okera’s technology into its Unity Catalog, its existing governance solution of data and AI assets. The company also noted that the acquisition will enable Databricks to expose additional APIs that its own data governance partners will be able to use to provide solutions to their customers.\\n\\nWith this acquisition, Databricks is also bringing Okera co-founder and CEO Nong Li on board. Li created the Apache Parquet data storage format and was actually briefly an engineer at Databricks between working at Cloudera and before starting Okera, where he was the founding CTO and became the CEO in February 2022.\\n\\n“As data continues to grow in volume, velocity, and variety across different applications, CIOs, CDOs, and CEOs across the board have to balance those two often conflicting initiatives – not to mention that historically, managing access policies across multiple clouds has been painful and time-consuming,” writes Li in today’s announcement. “Many organizations don’t have enough technical talent to manage access policies at scale, especially with the explosion of LLMs. What they need is a modern, AI-centric governance solution. We could not be more excited to join the Databricks team and to bring our expertise in building secure, scalable and simple governance solutions for some of the world’s most forward-thinking enterprises.”\\n\\nIf you know more about this acquisition, you can contact Frederic on Signal at (860) 208-3416 or by email (frederic@techcrunch.com). You can also reach us via SecureDrop.', metadata={'source': './news_articles\\\\05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt'}),\n",
       "  Document(page_content='Signaling that investments in the supply chain sector remain robust, Pando, a startup developing fulfillment management technologies, today announced that it raised $30 million in a Series B round, bringing its total raised to $45 million.\\n\\nIron Pillar and Uncorrelated Ventures led the round, with participation from existing investors Nexus Venture Partners, Chiratae Ventures and Next47. CEO and founder Nitin Jayakrishnan says that the new capital will be put toward expanding Pando’s global sales, marketing and delivery capabilities.\\n\\n“We will not expand into new industries or adjacent product areas,” he told TechCrunch in an email interview. “Great talent is the foundation of the business — we will continue to augment our teams at all levels of the organization. Pando is also open to exploring strategic partnerships and acquisitions with this round of funding.”\\n\\nPando was co-launched by Jayakrishnan and Abhijeet Manohar, who previously worked together at iDelivery, an India-based freight tech marketplace — and their first startup. The two saw firsthand manufacturers, distributors and retailers were struggling with legacy tech and point solutions to understand, optimize and manage their global logistics operations — or at least, that’s the story Jayakrishnan tells.\\n\\n“Supply chain leaders were trying to build their own tech and throwing people at the problem,” he said. “This caught our attention — we spent months talking to and building for enterprise users at warehouses, factories, freight yards and ports and eventually, in 2018, decided to start Pando to solve for global logistics through a software-as-a-service platform offering.”\\n\\nThere’s truth to what Jayakrishnan’s expressing about pent-up demand. According to a recent McKinsey survey, supply chain companies had — and have — a strong desire for tools that deliver greater supply chain visibility. Sixty-seven percent of respondents to the survey say that they’ve implemented dashboards for this purpose, while over half say that they’re investing in supply chain visibility services more broadly.\\n\\nPando aims to meet the need by consolidating supply chain data that resides in multiple silos within and outside of the enterprise, including data on customers, suppliers, logistics service providers, facilities and product SKUs. The platform provides various tools and apps for accomplishing different tasks across freight procurement, trade and transport management, freight audit and payment and document management, as well as dispatch planning and analytics.\\n\\nCustomers can customize the tools and apps or build their own using Pando’s APIs. This, along with the platform’s emphasis on no-code capabilities, differentiates Pando from incumbents like SAP, Oracle, Blue Yonder and E2Open, Jayakrishnan asserts.\\n\\n“Pando comes pre-integrated with leading enterprise resource planning (ERPs) systems and has ready APIs and a professional services team to integrate with any new ERPs and enterprise systems,” he added. “Pando’s no-code capabilities enable business users to customize the apps while maintaining platform integrity — reducing the need for IT resources for each customization.”\\n\\nPando also taps algorithms and forms of machine learning to make predictions around supply chain events. For example, the platform attempts to match customer orders with suppliers, customers through the “right” channel (in terms of aspects like cost and carbon footprint) and fulfillment strategy (e.g. mode of freight, carrier, etc.). Beyond this, Pando can detect anomalies among deliveries, orders and freight invoices and anticipate supply chain risk given demand and supply trends.\\n\\nPando isn’t the only vendor doing this. Altana, which bagged $100 million in venture capital last October, uses an AI system to connect to and learn from logistics and business-to-business data — creating a shared view of supply chain networks. Everstream, another Pando rival, offers its own dashboards for data analysis, integrated with existing ERP, transportation management and supplier relationship management systems.\\n\\nBut Pando has a compelling sales pitch, judging by its momentum. The company counts Fortune 500 manufacturers and retailers — including P&G, J&J, Valvoline, Castrol, Cummins, Siemens, Danaher and Accuride — among its customer base. Since the startup’s Series A in 2020, revenue has grown 8x while the number of customers has increased 5x, Jayakrishnan said.\\n\\nAsked whether he expects expansion to continue well into the future, given the signs of potential trouble on the horizon, Jayakrishnan seemed fairly optimistic. He pointed to a Deloitte survey that found that more than 70% of manufacturing companies have been impacted by supply chain disruptions in the past year, with 90% of those companies experiencing increased costs and declining productivity.\\n\\nThe result of those major disruptions? The digital logistics market is estimated to climb to $46.5 billion by 2025, per Markets and Markets — up from $17.4 billion in 2019. Crunchbase reports that investors poured more than $7 billion in seed through growth-stage rounds globally for supply chain-focused startups from January to October 2022, nearly eclipsing 2021’s record-setting levels.\\n\\n“Pando has a strong balance sheet and profit and loss statement, with an eye on profitable growth,” Jayakrishnan said. “We’re are scaling operations in North America, Europe and India with marquee customer wins and a network of strong partners … Pando is well-positioned to ride this growth wave, and drive supply chain agility for the 2030 economy.”', metadata={'source': './news_articles\\\\05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt'})]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Databricks has recently acquired Okera, a data governance platform. Databricks plans to integrate Okera's technology into its own platform, specifically its Unity Catalog. The acquisition will also enable Databricks to expose additional APIs that its data governance partners can use to provide solutions for their customers. \n",
      "\n",
      "\n",
      "Sources:\n",
      "./news_articles\\05-03-databricks-acquires-ai-centric-data-governance-platform-okera.txt\n",
      "./news_articles\\05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt\n"
     ]
    }
   ],
   "source": [
    "cited_answer(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r vecdb.zip ./vecdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.delete_collection()\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf vecdb/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
